==== C:\Users\katha\CalibraFlow\core\all_contents.txt ==== 
==== C:\Users\katha\CalibraFlow\core\all_contents.txt ==== 
==== C:\Users\katha\CalibraFlow\core\__init__.py ==== 
==== C:\Users\katha\CalibraFlow\core\anomalies\AnomalyOrchestrator.py ==== 
import logging
from typing import Sequence
import polars as pl
from core.interfaces.IAnomalyDetector import IAnomalyDetector

logger = logging.getLogger(__name__)

class AnomalyOrchestrator:
    # orchestrates anomaly detection on preprocessed datasets

    def __init__(self, detector: IAnomalyDetector) -> None:
        # dependency injected detector implementation
        self._detector = detector

    def fit_on_reference(
        self,
        df_reference: pl.DataFrame,
        feature_columns: Sequence[str],
    ) -> None:
        # train anomaly detector on (assumed) mostly normal reference data
        logger.info("Fitting anomaly detector on reference dataset")
        self._detector.fit(df_reference, feature_columns)

    def run_detection(
        self,
        df_target: pl.DataFrame,
        threshold: float,
    ) -> pl.DataFrame:
        # run anomaly detection on target dataset and return annotated dataframe
        logger.info(
            "Running anomaly detection (threshold=%s) on target dataset",
            threshold,
        )
        result = self._detector.detect(df_target, threshold)
        return result
==== C:\Users\katha\CalibraFlow\core\anomalies\IsolationForestDetector.py ==== 
import logging
from typing import Sequence
import polars as pl
import numpy as np
from sklearn.ensemble import IsolationForest
from core.interfaces.IAnomalyDetector import IAnomalyDetector

logger = logging.getLogger(__name__)

# numeric dtype set for polars
_NUMERIC_DTYPES = {
    pl.Int8, pl.Int16, pl.Int32, pl.Int64,
    pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,
    pl.Float32, pl.Float64,
}

def _is_numeric_dtype(dtype) -> bool:
    return dtype in _NUMERIC_DTYPES


class IsolationForestDetector(IAnomalyDetector):
    # wrapper around sklearn IsolationForest for multivariate anomaly detection

    def __init__(
        self,
        n_estimators: int = 100,
        contamination: float | None = 0.01,
        random_state: int | None = 42,
    ) -> None:
        self._model = IsolationForest(
            n_estimators=n_estimators,
            contamination=contamination,
            random_state=random_state,
        )
        self._feature_columns: Sequence[str] = []

    def fit(self, df: pl.DataFrame, feature_columns: Sequence[str]) -> None:
        if df.is_empty():
            raise ValueError("Cannot fit IsolationForest on empty dataframe")

        self._feature_columns = list(feature_columns)

        for col in self._feature_columns:
            if col not in df.columns:
                raise ValueError(f"Feature column '{col}' not in dataframe")

            if not _is_numeric_dtype(df[col].dtype):
                raise TypeError(f"Feature column '{col}' must be numeric")

        X = df.select(self._feature_columns).to_numpy()
        self._model.fit(X)

        logger.info(
            "Fitted IsolationForest on columns: %s (n_samples=%s)",
            self._feature_columns,
            X.shape[0],
        )

    def score(self, df: pl.DataFrame) -> pl.DataFrame:
        if not self._feature_columns:
            raise RuntimeError("IsolationForestDetector must be fitted before 'score'")

        for col in self._feature_columns:
            if col not in df.columns:
                raise ValueError(f"Feature column '{col}' not present at scoring time")

        X = df.select(self._feature_columns).to_numpy()

        # sklearn: more negative = more anomalous
        raw_scores = self._model.score_samples(X)
        anomaly_score = -raw_scores

        anomaly_score = anomaly_score - float(np.min(anomaly_score))
        max_val = float(np.max(anomaly_score))
        if max_val > 0:
            anomaly_score = anomaly_score / max_val

        scored = df.with_columns(
            pl.Series("anomaly_score", anomaly_score.tolist())
        )

        return scored

    def detect(self, df: pl.DataFrame, threshold: float) -> pl.DataFrame:
        scored = self.score(df)

        scored = scored.with_columns(
            (pl.col("anomaly_score") >= threshold).alias("is_anomaly")
        )

        return scored
==== C:\Users\katha\CalibraFlow\core\anomalies\ZScoreDetector.py ==== 
import logging
from typing import Dict, Sequence, Tuple
import polars as pl
from core.interfaces.IAnomalyDetector import IAnomalyDetector

logger = logging.getLogger(__name__)

_NUMERIC_DTYPES = {
    pl.Int8, pl.Int16, pl.Int32, pl.Int64,
    pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,
    pl.Float32, pl.Float64,
}

def _is_numeric_dtype(dtype) -> bool:
    return dtype in _NUMERIC_DTYPES


class ZScoreDetector(IAnomalyDetector):

    def __init__(self) -> None:
        self._stats: Dict[str, Tuple[float, float]] = {}
        self._feature_columns: Sequence[str] = []

    def fit(self, df: pl.DataFrame, feature_columns: Sequence[str]) -> None:
        if df.is_empty():
            raise ValueError("Cannot fit ZScoreDetector on empty dataframe")

        self._feature_columns = list(feature_columns)
        n = df.height

        for col in self._feature_columns:
            if col not in df.columns:
                raise ValueError(f"Feature column '{col}' not in dataframe")

            series = df[col]

            if not _is_numeric_dtype(series.dtype):
                raise TypeError(f"Feature column '{col}' must be numeric, got {series.dtype}")

            mean_val = float(series.mean())

            # stabilization for small sample sizes
            if n < 3:
                # minimal variance assumption → treat extreme values as anomalous
                std_val = 1.0
            else:
                std_val = float(series.std())
                if std_val == 0.0 or std_val != std_val:
                    std_val = 1.0

            self._stats[col] = (mean_val, std_val)

        logger.info("Fitted ZScoreDetector on columns: %s", self._feature_columns)

    def score(self, df: pl.DataFrame) -> pl.DataFrame:
        if not self._stats:
            raise RuntimeError("ZScoreDetector must be fitted before calling 'score'")

        working = df.clone()
        z_expressions = []

        for col in self._feature_columns:
            mean_val, std_val = self._stats[col]

            z_expr = (
                (pl.col(col) - mean_val)
                .cast(pl.Float64)
                .truediv(std_val)
                .alias(f"z_{col}")
            )
            z_expressions.append(z_expr)

        working = working.with_columns(z_expressions)

        abs_cols = [f"z_{c}" for c in self._feature_columns]

        working = working.with_columns(
            pl.concat_list([pl.col(c).abs() for c in abs_cols])
            .list.mean()
            .alias("anomaly_score")
        )

        return working

    def detect(self, df: pl.DataFrame, threshold: float) -> pl.DataFrame:
        scored = self.score(df)
        scored = scored.with_columns(
            (pl.col("anomaly_score") >= threshold).alias("is_anomaly")
        )
        return scored
==== C:\Users\katha\CalibraFlow\core\anomalies\__init__.py ==== 
==== C:\Users\katha\CalibraFlow\core\calibration\CalibrationStub.py ==== 
==== C:\Users\katha\CalibraFlow\core\features\RollingFeatureEngineer.py ==== 
import polars as pl
from typing import Sequence, List


class RollingFeatureEngineer:
    # Robust time-based rolling windows (Polars >= 1.20)

    def add_rolling_features(
        self,
        df: pl.DataFrame,
        feature_columns: Sequence[str],
        windows: Sequence[str],
    ) -> pl.DataFrame:

        if df.is_empty():
            return df

        if "timestamp" not in df.columns:
            raise ValueError("DataFrame must contain a 'timestamp' column")

        # ensure timestamp is datetime
        working = df.with_columns(
            pl.col("timestamp").str.strptime(pl.Datetime, strict=False)
        )

        exprs: List[pl.Expr] = []

        for col in feature_columns:
            if col not in working.columns:
                raise ValueError(f"Feature column '{col}' not in DataFrame")

            # require numeric dtype
            if working[col].dtype not in pl.NUMERIC_DTYPES:
                raise TypeError(f"Feature '{col}' must be numeric")

            for win in windows:
                # rolling returns list -> use .list.mean() / .list.std()
                roll = (
                    pl.col(col)
                    .rolling(index_column="timestamp", period=win, closed="right")
                )

                exprs.append(
                    roll.list.mean().alias(f"{col}_roll_mean_{win}")
                )
                exprs.append(
                    roll.list.std().alias(f"{col}_roll_std_{win}")
                )

        return working.with_columns(exprs)
==== C:\Users\katha\CalibraFlow\core\features\TimeFeatureEngineer.py ==== 
import polars as pl

class TimeFeatureEngineer:
    # add derived time-based features to a dataframe containing a datetime column 'timestamp'

    def add_time_features(self, df: pl.DataFrame) -> pl.DataFrame:
        if "timestamp" not in df.columns:
            raise ValueError("Expected column 'timestamp' in dataframe")

        # robust timestamp parsing
        working = df.with_columns(
            pl.col("timestamp")
            .str.strptime(pl.Datetime, strict=False)
            .alias("timestamp")
        )

        # season mapping helper
        def season_expr(col):
            return (
                pl.when(col.is_in([12, 1, 2])).then(0)  # winter
                .when(col.is_in([3, 4, 5])).then(1)     # spring
                .when(col.is_in([6, 7, 8])).then(2)     # summer
                .otherwise(3)                           # fall
            )

        # compute weekday in Python convention:
        # Monday=0, Sunday=6
        weekday_expr = (pl.col("timestamp").dt.weekday() - 1)

        working = working.with_columns([
            pl.col("timestamp").dt.hour().alias("hour"),
            weekday_expr.alias("day_of_week"),
            pl.col("timestamp").dt.month().alias("month"),
            pl.col("timestamp").dt.year().alias("year"),
            (weekday_expr >= 5).alias("is_weekend"),  # Saturday(5), Sunday(6)
            season_expr(pl.col("timestamp").dt.month()).alias("season"),
        ])

        return working
==== C:\Users\katha\CalibraFlow\core\interfaces\IAnomalyDetector.py ==== 
from abc import ABC, abstractmethod
from typing import Sequence
import polars as pl

class IAnomalyDetector(ABC):
    # contract for all anomaly detectors in the system

    @abstractmethod
    def fit(self, df: pl.DataFrame, feature_columns: Sequence[str]) -> None:
        # train the detector on (assumed) mostly normal data
        pass

    @abstractmethod
    def score(self, df: pl.DataFrame) -> pl.DataFrame:
        # compute anomaly scores for each row and return df with extra column 'anomaly_score'
        pass

    @abstractmethod
    def detect(self, df: pl.DataFrame, threshold: float) -> pl.DataFrame:
        # apply a threshold to 'anomaly_score' and return df with extra boolean column 'is_anomaly'
        pass
==== C:\Users\katha\CalibraFlow\core\interfaces\ICalibrationModel.py ==== 
==== C:\Users\katha\CalibraFlow\core\interfaces\IDataLoader.py ==== 
from abc import ABC, abstractmethod
from pathlib import Path
import polars as pl
from core.loaders.dataset_ids import DatasetId

class IDataLoader(ABC):
    # contract for all dataset loaders
    @abstractmethod
    def load_dataset(self, dataset_id: DatasetId) -> pl.DataFrame:
        pass
==== C:\Users\katha\CalibraFlow\core\interfaces\IDataPreprocessor.py ==== 
from abc import ABC, abstractmethod
import polars as pl

class IDataPreprocessor(ABC):
    # contract for all preprocessing components
    @abstractmethod
    def preprocess(self, df: pl.DataFrame) -> pl.DataFrame:
        pass
==== C:\Users\katha\CalibraFlow\core\interfaces\IFeatureEngineer.py ==== 
==== C:\Users\katha\CalibraFlow\core\interfaces\IVisualizer.py ==== 
==== C:\Users\katha\CalibraFlow\core\loaders\csv_dataset_loader.py ==== 
from pathlib import Path
import logging
from typing import Set
import polars as pl

from core.interfaces.IDataLoader import IDataLoader
from .dataset_ids import DatasetId
from .dataset_config import DATASET_REGISTRY, DatasetConfig

logger = logging.getLogger(__name__)

class CsvDatasetLoader(IDataLoader):
    # single responsibility: reading datasets using dataset-specific configs
    def __init__(self, base_path: Path) -> None:
        self._base_path = base_path

    def load_dataset(self, dataset_id: DatasetId) -> pl.DataFrame:
        config: DatasetConfig = self._get_config(dataset_id)
        file_path = self._base_path / config.relative_path

        logger.debug("Loading dataset '%s' from '%s'", dataset_id.value, file_path)

        if not file_path.exists():
            logger.error("Dataset file not found: %s", file_path)
            raise FileNotFoundError(f"Dataset file not found: {file_path}")

        scan = pl.scan_csv(
            file_path,
            has_header=config.has_header,
            separator=config.delimiter,
            encoding=config.encoding,
            null_values=config.null_values,
        )

        schema = scan.collect_schema()
        available_columns: Set[str] = set(schema.names())

        for col in config.parse_dates:
            if col in available_columns:
                scan = scan.with_columns(
                    pl.col(col).str.strptime(pl.Datetime, strict=False)
                )
                logger.debug("Parsed datetime column '%s' for dataset '%s'",
                             col, dataset_id.value)
            else:
                logger.warning("Configured datetime column '%s' not found in dataset '%s'",
                               col, dataset_id.value)

        df = scan.collect()

        if config.rename_columns:
            df = df.rename(config.rename_columns)
            logger.debug("Applied column renames for dataset '%s': %s",
                         dataset_id.value, config.rename_columns)

        missing = set(config.required_columns) - set(df.columns)
        if missing:
            logger.error("Missing required columns for dataset '%s': %s",
                         dataset_id.value, sorted(missing))
            raise ValueError(f"Missing required columns for {dataset_id.value}: {sorted(missing)}")

        if config.dtypes:
            for col_name, dtype in config.dtypes.items():
                if col_name in df.columns:
                    df = df.with_columns(pl.col(col_name).cast(dtype))
                    logger.debug("Cast column '%s' to dtype '%s' for dataset '%s'",
                                 col_name, dtype, dataset_id.value)
                else:
                    logger.warning(
                        "Configured dtype for column '%s', but column not present in dataset '%s'",
                        col_name, dataset_id.value,
                    )

        logger.info(
            "Loaded dataset '%s' (rows=%s, cols=%s)",
            dataset_id.value, df.height, df.width,
        )

        return df

    def _get_config(self, dataset_id: DatasetId) -> DatasetConfig:
        try:
            return DATASET_REGISTRY[dataset_id]
        except KeyError as exc:
            logger.error("No DatasetConfig registered for id=%s", dataset_id.value)
            raise KeyError(f"No DatasetConfig registered for id={dataset_id.value}") from exc
==== C:\Users\katha\CalibraFlow\core\loaders\dataset_config.py ==== 
# core/loaders/dataset_config.py
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Mapping, Optional
import polars as pl
from .dataset_ids import DatasetId

@dataclass(frozen=True)
class DatasetConfig:
    # describes how to read and normalize a dataset
    dataset_id: DatasetId
    relative_path: Path
    parse_dates: List[str]
    rename_columns: Mapping[str, str]
    required_columns: List[str]
    dtypes: Optional[Mapping[str, pl.DataType]] = None

    # extended loader settings
    delimiter: str = ","  # default CSV separator
    has_header: bool = True  # default header row
    encoding: str = "utf8"  # default encoding
    null_values: Optional[list[str]] = None  # optional null markers

# base folder for synthetic datasets
BASE_SYN = Path("data/syntetische_daten_heilbronn_2021_2023")

DATASET_REGISTRY: Dict[DatasetId, DatasetConfig] = {
    DatasetId.AIR_QUALITY_REFERENCE: DatasetConfig(
        dataset_id=DatasetId.AIR_QUALITY_REFERENCE,
        relative_path=BASE_SYN / "air_quality_reference_values_germany.csv",
        parse_dates=[],
        rename_columns={"pollutant": "pollutant"},
        required_columns=["pollutant"],
        dtypes=None,
    ),
    DatasetId.AIR_QUALITY_RAW: DatasetConfig(
        dataset_id=DatasetId.AIR_QUALITY_RAW,
        relative_path=BASE_SYN / "heilbronn_air_quality_2021_2023.csv",
        parse_dates=["timestamp"],
        rename_columns={"timestamp": "timestamp", "station_id": "station_id"},
        required_columns=["timestamp", "station_id"],
        dtypes=None,
    ),
    DatasetId.AIR_QUALITY_CALIBRATED: DatasetConfig(
        dataset_id=DatasetId.AIR_QUALITY_CALIBRATED,
        relative_path=BASE_SYN / "heilbronn_air_quality_2021_2023_calibrated.csv",
        parse_dates=["timestamp"],
        rename_columns={"timestamp": "timestamp", "station_id": "station_id"},
        required_columns=["timestamp", "station_id"],
        dtypes=None,
    ),
    DatasetId.NOISE_RAW: DatasetConfig(
        dataset_id=DatasetId.NOISE_RAW,
        relative_path=BASE_SYN / "heilbronn_noise_2021_2023.csv",
        parse_dates=["timestamp"],
        rename_columns={"timestamp": "timestamp", "sensor_id": "sensor_id"},
        required_columns=["timestamp", "sensor_id"],
        dtypes=None,
    ),
    DatasetId.NOISE_CALIBRATED: DatasetConfig(
        dataset_id=DatasetId.NOISE_CALIBRATED,
        relative_path=BASE_SYN / "heilbronn_noise_2021_2023_calibrated.csv",
        parse_dates=["timestamp"],
        rename_columns={"timestamp": "timestamp", "sensor_id": "sensor_id"},
        required_columns=["timestamp", "sensor_id"],
        dtypes=None,
    ),
    DatasetId.WEATHER_RAW: DatasetConfig(
        dataset_id=DatasetId.WEATHER_RAW,
        relative_path=BASE_SYN / "heilbronn_weather_2021_2023.csv",
        parse_dates=["timestamp"],
        rename_columns={"timestamp": "timestamp", "station_id": "station_id"},
        required_columns=["timestamp", "station_id"],
        dtypes=None,
    ),
    DatasetId.WEATHER_CALIBRATED: DatasetConfig(
        dataset_id=DatasetId.WEATHER_CALIBRATED,
        relative_path=BASE_SYN / "heilbronn_weather_2021_2023_calibrated.csv",
        parse_dates=["timestamp"],
        rename_columns={"timestamp": "timestamp", "station_id": "station_id"},
        required_columns=["timestamp", "station_id"],
        dtypes=None,
    ),
}
==== C:\Users\katha\CalibraFlow\core\loaders\dataset_ids.py ==== 
from enum import Enum

class DatasetId(str, Enum):
    # logical dataset identifiers
    AIR_QUALITY_REFERENCE = "air_quality_reference"
    AIR_QUALITY_RAW = "air_quality_raw"
    AIR_QUALITY_CALIBRATED = "air_quality_calibrated"
    NOISE_RAW = "noise_raw"
    NOISE_CALIBRATED = "noise_calibrated"
    WEATHER_RAW = "weather_raw"
    WEATHER_CALIBRATED = "weather_calibrated"

    LUBW_MINUTE = "lubw_minute_data"
    AIRUP_SONT_A = "airup_sont_a_minute"
    AIRUP_SONT_C = "airup_sont_c_minute"
==== C:\Users\katha\CalibraFlow\core\loaders\loader_orchestrator.py ==== 
from typing import Dict
import logging
import polars as pl

from core.interfaces.IDataLoader import IDataLoader
from .dataset_ids import DatasetId

logger = logging.getLogger(__name__)

class LoaderOrchestrator:
    # orchestrates loading of logical datasets via injected loader
    def __init__(self, dataset_loader: IDataLoader) -> None:
        self._dataset_loader = dataset_loader

    def load_all(self, dataset_ids: list[DatasetId]) -> Dict[DatasetId, pl.DataFrame]:
        result: Dict[DatasetId, pl.DataFrame] = {}
        for ds_id in dataset_ids:
            logger.info("Loading dataset '%s'", ds_id.value)
            df = self._dataset_loader.load_dataset(ds_id)
            result[ds_id] = df
            logger.debug(
                "Dataset '%s' loaded into orchestrator with rows=%s, cols=%s",
                ds_id.value, df.height, df.width,
            )
        logger.info("Loaded %s datasets via orchestrator", len(result))
        return result
==== C:\Users\katha\CalibraFlow\core\loaders\__init__.py ==== 
==== C:\Users\katha\CalibraFlow\core\pipeline\AnalysisPipeline.py ==== 
==== C:\Users\katha\CalibraFlow\core\preprocessing\airup_sensor_preprocessor.py ==== 
import polars as pl
from .base_preprocessor import BasePreprocessor

class AirUpSensorPreprocessor(BasePreprocessor):
    def _select_columns(self, df: pl.DataFrame) -> pl.DataFrame:
        # keep only meaningful columns for analytics
        keep = [
            c for c in df.columns 
            if not c.startswith(("RAW_OPC", "RAW_ADC", "Laser", "Heater", "Fan"))
        ]
        return df.select(keep)

    def _resolve_timestamps(self, df: pl.DataFrame) -> pl.DataFrame:
        if "timestamp_gps" in df.columns and df["timestamp_gps"].dtype != pl.Utf8:
            return df.with_columns(
                pl.col("timestamp_gps").cast(pl.Datetime).alias("timestamp")
            )
        if "timestamp_hr" in df.columns:
            return df.with_columns(
                pl.col("timestamp_hr").str.strptime(pl.Datetime).alias("timestamp")
            )
        if "timestamp" in df.columns:
            return df.with_columns(
                pl.col("timestamp").cast(pl.Int64).cast(pl.Datetime, "ms")
            )
        return df

    def _validate_ranges(self, df: pl.DataFrame) -> pl.DataFrame:
        rules = {
            "NO": (0, None),
            "NO2": (0, None),
            "O3": (0, None),
            "temperature": (-40, 80),
            "humidity": (0, 100),
        }
        for col, (min_v, max_v) in rules.items():
            if col in df.columns:
                if min_v is not None:
                    df = df.filter(pl.col(col) >= min_v)
                if max_v is not None:
                    df = df.filter(pl.col(col) <= max_v)
        return df
==== C:\Users\katha\CalibraFlow\core\preprocessing\base_preprocessor.py ==== 
import logging
from typing import Callable, List
import polars as pl
from core.interfaces.IDataPreprocessor import IDataPreprocessor

logger = logging.getLogger(__name__)

class BasePreprocessor(IDataPreprocessor):
    def __init__(self) -> None:
        self._steps: List[Callable[[pl.DataFrame], pl.DataFrame]] = [
            self._select_columns,
            self._resolve_timestamps,
            self._normalize_units,
            self._validate_ranges,
            self._handle_missing,
            self._finalize,
        ]

    def get_steps(self) -> List[Callable[[pl.DataFrame], pl.DataFrame]]:
        return list(self._steps)

    def preprocess(self, df: pl.DataFrame) -> pl.DataFrame:
        if df is None:
            raise ValueError("Input dataframe cannot be None")

        current = df

        for step in self._steps:
            step_name = step.__name__
            logger.debug("Entering step '%s'", step_name)

            next_df = step(current)

            if next_df is None:
                raise RuntimeError(
                    f"Preprocessing step '{step_name}' returned None. "
                    "Each step must return a polars DataFrame."
                )

            if not isinstance(next_df, pl.DataFrame):
                raise TypeError(
                    f"Step '{step_name}' must return a pl.DataFrame, "
                    f"but returned {type(next_df)}"
                )

            logger.debug(
                "Exiting step '%s' (rows=%s, cols=%s)",
                step_name,
                next_df.height,
                next_df.width,
            )

            current = next_df

        return current

    def _select_columns(self, df: pl.DataFrame) -> pl.DataFrame:
        return df

    def _resolve_timestamps(self, df: pl.DataFrame) -> pl.DataFrame:
        return df

    def _normalize_units(self, df: pl.DataFrame) -> pl.DataFrame:
        return df

    def _validate_ranges(self, df: pl.DataFrame) -> pl.DataFrame:
        return df

    def _handle_missing(self, df: pl.DataFrame) -> pl.DataFrame:
        return df

    def _finalize(self, df: pl.DataFrame) -> pl.DataFrame:
        return df
==== C:\Users\katha\CalibraFlow\core\preprocessing\lubw_minute_preprocessor.py ==== 
import polars as pl
from .base_preprocessor import BasePreprocessor
from core.preprocessing.utils.time_utils import parse_timestamp

class LUBWMinutePreprocessor(BasePreprocessor):

    def _resolve_timestamps(self, df: pl.DataFrame) -> pl.DataFrame:
        if "timestamp" in df.columns:
            return df.with_columns(
                parse_timestamp(pl.col("timestamp")).alias("timestamp")
            )

        if "Hour" in df.columns:
            return df.with_columns(
                parse_timestamp(pl.col("Hour")).alias("timestamp")
            )

        return df

    def _validate_ranges(self, df: pl.DataFrame) -> pl.DataFrame:
        numeric_columns = [c for c in df.columns if c not in ("timestamp", "flag")]
        for col in numeric_columns:
            df = df.filter(pl.col(col).is_not_null())
            df = df.filter(pl.col(col) >= 0)
        return df
==== C:\Users\katha\CalibraFlow\core\preprocessing\preprocessing_config.py ==== 
from core.loaders.dataset_ids import DatasetId
from .synthetic_air_quality_preprocessor import SyntheticAirQualityPreprocessor
from .lubw_minute_preprocessor import LUBWMinutePreprocessor
from .airup_sensor_preprocessor import AirUpSensorPreprocessor

PREPROCESSOR_REGISTRY = {
    DatasetId.AIR_QUALITY_REFERENCE: SyntheticAirQualityPreprocessor(),
    DatasetId.AIR_QUALITY_RAW: SyntheticAirQualityPreprocessor(),
    DatasetId.AIR_QUALITY_CALIBRATED: SyntheticAirQualityPreprocessor(),
    DatasetId.NOISE_RAW: SyntheticAirQualityPreprocessor(),
    DatasetId.NOISE_CALIBRATED: SyntheticAirQualityPreprocessor(),
    DatasetId.WEATHER_RAW: SyntheticAirQualityPreprocessor(),
    DatasetId.WEATHER_CALIBRATED: SyntheticAirQualityPreprocessor(),

    DatasetId.LUBW_MINUTE: LUBWMinutePreprocessor(),
    DatasetId.AIRUP_SONT_A: AirUpSensorPreprocessor(),
    DatasetId.AIRUP_SONT_C: AirUpSensorPreprocessor(),
}
==== C:\Users\katha\CalibraFlow\core\preprocessing\preprocessing_orchestrator.py ==== 
import polars as pl
from core.loaders.dataset_ids import DatasetId
from core.interfaces.IDataPreprocessor import IDataPreprocessor

class PreprocessingOrchestrator:
    def __init__(self, registry: dict[DatasetId, IDataPreprocessor]) -> None:
        self._registry = registry

    def preprocess(self, dataset_id: DatasetId, df: pl.DataFrame) -> pl.DataFrame:
        if dataset_id not in self._registry:
            raise ValueError(f"No preprocessor registered for dataset {dataset_id}")

        processor = self._registry[dataset_id]

        if not isinstance(processor, IDataPreprocessor):
            raise TypeError(
                f"Registered processor for {dataset_id} must implement IDataPreprocessor"
            )

        return processor.preprocess(df)
==== C:\Users\katha\CalibraFlow\core\preprocessing\synthetic_air_quality_preprocessor.py ==== 
import polars as pl
from .base_preprocessor import BasePreprocessor
from core.preprocessing.utils.time_utils import parse_timestamp

class SyntheticAirQualityPreprocessor(BasePreprocessor):

    def _select_columns(self, df: pl.DataFrame) -> pl.DataFrame:
        return df

    def _resolve_timestamps(self, df: pl.DataFrame) -> pl.DataFrame:
        if "timestamp" in df.columns:
            return df.with_columns(
                parse_timestamp(pl.col("timestamp")).alias("timestamp")
            )
        return df
==== C:\Users\katha\CalibraFlow\core\preprocessing\utils\time_utils.py ==== 
import polars as pl
import logging

logger = logging.getLogger(__name__)

def parse_timestamp(expr: pl.Expr) -> pl.Expr:
    # best effort parsing for timestamps represented as strings or numeric epochs

    # step 1: try to parse ISO-like string timestamps (safe, strict=False)
    parsed_str = expr.str.strptime(pl.Datetime, strict=False)

    # step 2: try to interpret values as integer epochs
    # numeric_int ist entweder eine Ganzzahl oder null, aber wirft keine Fehler
    numeric_int = expr.cast(pl.Int64, strict=False)

    # epoch in millisekunden direkt zu datetime[ms]
    parsed_epoch_ms = numeric_int.cast(pl.Datetime("ms"), strict=False)

    # epoch in sekunden: erst in millisekunden hochskalieren, dann nach datetime[ms]
    parsed_epoch_s = numeric_int.mul(1000).cast(pl.Datetime("ms"), strict=False)

    # wenn der numerische Wert groß genug ist, behandeln wir ihn als ms, sonst als s
    numeric_parsed = (
        pl.when(numeric_int >= 1_000_000_000_000)
        .then(parsed_epoch_ms)
        .otherwise(parsed_epoch_s)
    )

    # finale Strategie:
    # 1) wenn String-Parsing geklappt hat, diesen Wert verwenden
    # 2) sonst numerische Epoch-Interpretation
    # fehlgeschlagene Werte bleiben null, um den Typ Datetime konsistent zu halten
    result = parsed_str.fill_null(numeric_parsed)

    return result
==== C:\Users\katha\CalibraFlow\core\visualization\SimplePlotter.py ==== 
