==== C:\Users\katha\CalibraFlow\tests\loaders\all_contents.txt ==== 
==== C:\Users\katha\CalibraFlow\tests\loaders\all_contents.txt ==== 
==== C:\Users\katha\CalibraFlow\tests\loaders\test_airup_dataset_loader.py ==== 
import polars as pl
from pathlib import Path
from core.loaders.airup_dataset_loader import AirUpDatasetLoader
from core.loaders.dataset_ids import DatasetId


def test_airup_loader_loads_and_concatenates_multiple_files(tmp_path: Path, patched_registry):
    data_dir = tmp_path / "sont_a"
    data_dir.mkdir()

    header = "pm1,pm25,pm10,sht_humid,sht_temp,CO,NO,NO2,O3,timestamp_hr\n"
    row1 = "1.0,2.0,3.0,40,20,1,2,3,4,2024-11-13 08:00:00\n"
    row2 = "1.1,2.1,3.1,41,21,1,2,3,4,2024-11-13 08:01:00\n"

    (data_dir / "airup_sont_a_avg_every_minute_data.log.2024-11-13").write_text(header + row1)
    (data_dir / "airup_sont_a_avg_every_minute_data.log.2024-11-14").write_text(header + row2)

    patched_registry[DatasetId.AIRUP_SONT_A] = patched_registry[DatasetId.AIRUP_SONT_A].__class__(
        dataset_id=DatasetId.AIRUP_SONT_A,
        relative_path=data_dir.relative_to(tmp_path),
        parse_dates=[],
        rename_columns={"sht_humid": "humidity", "sht_temp": "temperature"},
        required_columns=[
            "timestamp_hr", "pm1", "pm25", "pm10",
            "CO", "NO", "NO2", "O3", "humidity", "temperature"
        ],
        dtypes=None,
    )

    loader = AirUpDatasetLoader(tmp_path)
    df = loader.load_dataset(DatasetId.AIRUP_SONT_A)

    assert df.height == 2
    assert set(df.columns) >= {"pm1", "pm25", "pm10", "humidity", "temperature", "timestamp_hr"}
    assert df["pm1"].to_list() == [1.0, 1.1]
    assert df["humidity"].to_list() == [40, 41]
==== C:\Users\katha\CalibraFlow\tests\loaders\test_csv_dataset_loader.py ==== 
import polars as pl
from pathlib import Path
from core.loaders.csv_dataset_loader import CsvDatasetLoader
from core.loaders.dataset_ids import DatasetId


def test_csv_loader_reads_single_file(tmp_path: Path, patched_registry):
    test_file = tmp_path / "file.csv"
    test_file.write_text(
        "timestamp,station_id,value\n"
        "2020-01-01 00:00:00,1,10\n"
    )

    patched_registry[DatasetId.AIR_QUALITY_RAW] = patched_registry[DatasetId.AIR_QUALITY_RAW].__class__(
        dataset_id=DatasetId.AIR_QUALITY_RAW,
        relative_path=test_file.relative_to(tmp_path),
        parse_dates=["timestamp"],
        rename_columns={"timestamp": "timestamp", "station_id": "station_id"},
        required_columns=["timestamp", "station_id"],
        dtypes=None,
    )

    loader = CsvDatasetLoader(tmp_path)
    df = loader.load_dataset(DatasetId.AIR_QUALITY_RAW)

    assert df.height == 1
    assert df["value"][0] == 10
==== C:\Users\katha\CalibraFlow\tests\loaders\test_loader_integration_full.py ==== 
import polars as pl
from pathlib import Path
from core.loaders.csv_dataset_loader import CsvDatasetLoader
from core.loaders.airup_dataset_loader import AirUpDatasetLoader
from core.loaders.loader_orchestrator import LoaderOrchestrator
from core.loaders.dataset_ids import DatasetId


def test_full_loader_integration(tmp_path: Path, patched_registry):
    lubw_dir = tmp_path / "lubw"
    lubw_dir.mkdir()
    lubw_file = lubw_dir / "minute_data_lubw_full.csv"
    lubw_file.write_text(
        "datetime,NO2,O3,PM10,PM2p5,TEMP,RLF,p-Luft,NSCH,WIR,WIV\n"
        "2024-11-14 00:01:00,24,3,23,22,5,90,1012,0,180,1"
    )

    patched_registry[DatasetId.LUBW_MINUTE] = patched_registry[DatasetId.LUBW_MINUTE].__class__(
        dataset_id=DatasetId.LUBW_MINUTE,
        relative_path=lubw_file.relative_to(tmp_path),
        parse_dates=[],
        rename_columns={"datetime": "timestamp"},
        required_columns=["timestamp", "NO2", "O3", "PM10", "PM2p5", "TEMP", "RLF", "p-Luft", "NSCH", "WIR", "WIV"],
        dtypes=None,
    )

    a_dir = tmp_path / "sont_a"
    a_dir.mkdir()
    a_file = a_dir / "airup_sont_a_avg_every_minute_data.log.2024-11-13"
    a_file.write_text(
        "pm1,pm25,pm10,sht_humid,sht_temp,CO,NO,NO2,O3,timestamp_hr\n"
        "1,2,3,40,20,1,2,3,4,2024-11-13 10:00:00"
    )

    patched_registry[DatasetId.AIRUP_SONT_A] = patched_registry[DatasetId.AIRUP_SONT_A].__class__(
        dataset_id=DatasetId.AIRUP_SONT_A,
        relative_path=a_dir.relative_to(tmp_path),
        parse_dates=[],
        rename_columns={"sht_humid": "humidity", "sht_temp": "temperature"},
        required_columns=[
            "timestamp_hr", "pm1", "pm25", "pm10",
            "CO", "NO", "NO2", "O3", "humidity", "temperature"
        ],
        dtypes=None,
    )

    csv_loader = CsvDatasetLoader(tmp_path)
    air_loader = AirUpDatasetLoader(tmp_path)

    orch = LoaderOrchestrator(default_loader=csv_loader, loader_overrides={
        DatasetId.AIRUP_SONT_A: air_loader
    })

    df_lubw = orch.load(DatasetId.LUBW_MINUTE)
    df_air = orch.load(DatasetId.AIRUP_SONT_A)

    assert df_lubw.height == 1
    assert df_air.height == 1
    assert "timestamp" in df_lubw.columns
    assert df_air["pm1"][0] == 1
    assert df_air["humidity"][0] == 40
==== C:\Users\katha\CalibraFlow\tests\loaders\test_loader_orchestrator.py ==== 
import polars as pl
import pytest
from pathlib import Path

from core.loaders.csv_dataset_loader import CsvDatasetLoader
from core.loaders.loader_orchestrator import LoaderOrchestrator
from core.loaders.dataset_ids import DatasetId


@pytest.fixture(scope="session")
def project_root() -> Path:
    return Path(__file__).resolve().parents[1]


@pytest.fixture(scope="session")
def synthetic_dataset_ids() -> list[DatasetId]:
    return [
        DatasetId.AIR_QUALITY_REFERENCE,
        DatasetId.AIR_QUALITY_RAW,
        DatasetId.AIR_QUALITY_CALIBRATED,
        DatasetId.NOISE_RAW,
        DatasetId.NOISE_CALIBRATED,
        DatasetId.WEATHER_RAW,
        DatasetId.WEATHER_CALIBRATED,
    ]


@pytest.fixture(scope="session")
def orchestrator(project_root: Path) -> LoaderOrchestrator:
    loader = CsvDatasetLoader(project_root)
    return LoaderOrchestrator(default_loader=loader)


def test_load_all_returns_expected_keys(orchestrator, synthetic_dataset_ids):
    result = orchestrator.load_all(synthetic_dataset_ids)
    assert set(result.keys()) == set(synthetic_dataset_ids)


def test_each_synthetic_dataset_is_non_empty_dataframe(orchestrator, synthetic_dataset_ids):
    result = orchestrator.load_all(synthetic_dataset_ids)
    for ds_id, df in result.items():
        assert isinstance(df, pl.DataFrame)
        assert df.height > 0
        assert df.width > 0


def test_timestamp_column_is_datetime_where_configured(orchestrator, synthetic_dataset_ids):
    result = orchestrator.load_all(synthetic_dataset_ids)

    datasets_with_timestamp = {
        DatasetId.AIR_QUALITY_RAW,
        DatasetId.AIR_QUALITY_CALIBRATED,
        DatasetId.NOISE_RAW,
        DatasetId.NOISE_CALIBRATED,
        DatasetId.WEATHER_RAW,
        DatasetId.WEATHER_CALIBRATED,
    }

    for ds_id in synthetic_dataset_ids:
        if ds_id not in datasets_with_timestamp:
            continue

        df = result[ds_id]
        assert "timestamp" in df.columns
        assert df["timestamp"].dtype == pl.Datetime
